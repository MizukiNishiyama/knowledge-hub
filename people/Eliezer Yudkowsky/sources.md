# Eliezer Yudkowsky - 出典・参考リンク

> generated_at: 2026-02-25

## 一次情報

- [Machine Intelligence Research Institute（MIRI）公式サイト](https://intelligence.org/) - MIRIの研究・戦略・出版物の公式掲載先
- [Eliezer Yudkowsky - MIRI プロフィール](https://intelligence.org/team/eliezer-yudkowsky/) - 公式経歴・役職
- [Rationality: A-Z（The Sequences）全文](https://www.lesswrong.com/rationality) - LessWrongに掲載されたSequencesの全文
- [Coherent Extrapolated Volition（原論文PDF）](https://intelligence.org/files/CEV.pdf) - 2004年に発表されたAI整合性フレームワークの原典
- [Harry Potter and the Methods of Rationality](https://www.lesswrong.com/hpmor) - 合理主義思想を普及させたフィクション作品の全文
- [Lex Fridman Podcast #368 - Eliezer Yudkowsky](https://lexfridman.com/eliezer-yudkowsky/) - AIの危険性と人類文明の終焉に関する3時間超のインタビュー（2023年）
- [Dwarkesh Patel Podcast - Eliezer Yudkowsky](https://www.dwarkesh.com/p/eliezer-yudkowsky) - AI安全性、LLMの整合性、知性の本質に関する4時間のインタビュー（2023年）

## 主張の出典

1. **「誰かがそれを作れば、全員が死ぬ」**
   - [If Anyone Builds It, Everyone Dies - AI Frontiers Summary](https://ai-frontiers.org/articles/summary-of-if-anyone-builds-it-everyone-dies) - 2025年出版の共著書（Nate Soaresとの共著）の要約
   - [Eliezer Yudkowsky - Wikipedia](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) - 書籍の概要と文脈

2. **「AIモラトリアム6ヶ月では全く足りない」**
   - [TIME - Pausing AI Developments Isn't Enough. We Need to Shut It All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) - 2023年3月のTIME誌寄稿。FLI公開書簡への署名拒否と、より強力な措置の提唱
   - [LessWrong - Eliezer Yudkowsky's Letter in Time Magazine](https://www.lesswrong.com/posts/eo8odvou4efc9syrv/eliezer-yudkowsky-s-letter-in-time-magazine) - コミュニティによる記事分析

3. **「不正なデータセンターを空爆で破壊する覚悟を」**
   - [TIME - Pausing AI Developments Isn't Enough. We Need to Shut It All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) - 軍事的執行を含む提案の原典
   - [DCD - Leading AI alignment researcher pens Time piece calling for ban on large GPU clusters](https://www.datacenterdynamics.com/en/news/be-willing-to-destroy-a-rogue-data-center-by-airstrike-leading-ai-alignment-researcher-pens-time-piece-calling-for-ban-on-large-gpu-clusters/) - 提案に対する報道

4. **「整合性研究は間に合わない」**
   - [MIRI 2024 Mission and Strategy Update](https://intelligence.org/2024/01/04/miri-2024-mission-and-strategy-update/) - 技術研究から政策・コミュニケーションへの戦略転換の公式発表
   - [MIRI 2024 End-of-Year Update](https://intelligence.org/2024/12/02/miris-2024-end-of-year-update/) - 転換後の進捗報告

5. **「Coherent Extrapolated Volition」（整合的外挿意志）**
   - [Coherent Extrapolated Volition（原論文PDF）](https://intelligence.org/files/CEV.pdf) - 2004年の原典
   - [LessWrong Wiki - Coherent Extrapolated Volition](https://www.lesswrong.com/w/coherent-extrapolated-volition) - 概念の解説と議論

## MIRI戦略・組織関連

- [MIRI 2024 Communications Strategy](https://intelligence.org/2024/05/29/miri-2024-communications-strategy/) - コミュニケーション戦略の詳細
- [MIRI Communications Team 2024 Recap](https://intelligence.org/2025/04/03/miri-communications-team-2024-recap/) - 2024年のコミュニケーション活動の振り返り
- [MIRI - All Publications](https://intelligence.org/all-publications/) - MIRIの全出版物リスト
- [Machine Intelligence Research Institute - Wikipedia](https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute) - MIRIの歴史・組織概要

## 人物像・批評・分析

- [Eliezer Yudkowsky - Wikipedia](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) - 包括的な経歴・活動記録
- [Mind and Iron: The Eliezer Yudkowsky Paradox](https://mindandiron.substack.com/p/mind-and-iron-the-eliezer-yudkowsky) - Yudkowskyのパラドクスに関する分析
- [Lawfare - The Case for AI Doom Rests on Three Unsettled Questions](https://www.lawfaremedia.org/article/the-case-for-ai-doom-rests-on-three-unsettled-questions) - AI破滅論の論拠に対する法学的分析
- [Reason - What Eliezer Yudkowsky's AI doom predictions get wrong](https://reason.com/2026/02/01/superintelligent-ai-is-not-coming-to-kill-you/) - AI破滅論への反論（2026年）
- [LessWrong - Contra Yudkowsky on AI Doom](https://www.lesswrong.com/posts/Lwy7XKsDEEkjskZ77/contra-yudkowsky-on-ai-doom) - コミュニティ内部からの反論

## 決定理論研究

- [Functional Decision Theory（MIRI論文）](https://intelligence.org/2017/10/22/fdt/) - Yudkowsky・Soares共著の機能的決定理論に関する論文
