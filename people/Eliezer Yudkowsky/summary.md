# Eliezer Yudkowsky

> generated_at: 2026-02-25

## 1. 基本情報

- **人名**: Eliezer Yudkowsky（エリエゼル・ユドコウスキー）
- **所属**: Machine Intelligence Research Institute（MIRI）
- **役職**: 共同創設者・上席研究員
- **誕生年**: 1979年

## 2. ラベル（大分類）

- 思想家
- 研究者
- 活動家

## 3. 小ラベル（テーマ軸）

- 生成AI
- 社会思想
- 教育

## 4. 概要

AI整合性研究の創始者であり合理主義運動の思想的指導者

## 5. 詳細

独学でAI安全性研究を開拓し、2000年にMIRIを設立。LessWrongで「Sequences」を執筆し合理主義コミュニティを形成。2023年にTIME誌でAI開発の即時停止を主張し、2025年には共著『If Anyone Builds It, Everyone Dies』を出版。AI整合性問題を主流の議論に押し上げた中心人物。

## 6. 行動原理（実現したい社会・信念）

- **根本信念**: 超知能AIの開発は、整合性問題が解決されない限り人類の存亡に関わるリスクであり、「最初の試行で失敗すればやり直しはない」。安全性の担保なき開発は人類絶滅につながると確信している
- **目指す社会**: 人類がAIの制御を維持し、超知能AIが人間の価値観と整合した形で実現される世界。その前提として、十分な安全性研究が完了するまでフロンティアAI開発を停止する国際合意の形成を求めている
- **意思決定原理**: ベイズ的合理主義に基づく意思決定。不確実性下でも期待値最大化を追求し、existential risk（存亡リスク）の回避を最優先とする。「P(doom)」（破滅確率）の見積もりに基づく行動選択
- **哲学的姿勢**: 認知バイアスの体系的排除、証拠に基づく信念更新、そして「正しいことを正しいと言う」知的誠実さを重視する。社会的圧力や楽観バイアスに抗してでも、自らの確率推定に従って行動する

## 7. 興味深い主張

1. **「誰かがそれを作れば、全員が死ぬ」**: 現在の技術的理解に基づく超知能AI開発は、制御不能に陥り人類滅亡に至ると主張。2025年の共著書タイトルそのものが主張を端的に表す（sources.md #1）
2. **「AIモラトリアム6ヶ月では全く足りない」**: 2023年のFLI公開書簡への署名を拒否。理由は「問題の深刻さを過小評価し、要求が少なすぎる」ため。代わりにTIME誌で無期限停止と国際的な軍事的執行を含む提案を行った（sources.md #2）
3. **「不正なデータセンターを空爆で破壊する覚悟を」**: AI開発停止の国際合意を破る施設に対し、軍事的手段を含む執行措置を提唱。その過激さゆえに大きな議論を呼んだ（sources.md #3）
4. **「整合性研究は間に合わない」**: MIRI自身が2024年に技術的整合性研究からの撤退を発表。研究が「時間内に成功する可能性が極めて低い」と判断し、政策・コミュニケーション重視に転換した（sources.md #4）
5. **「Coherent Extrapolated Volition」（整合的外挿意志）**: AIの目標を「人類がより多くを知り、より速く考え、より望ましい自分になれたとき何を欲するか」から導出すべきとする2004年の理論。AI整合性の初期フレームワークとして影響力を持つ（sources.md #5）

## 8. 参考にできる点

- **Sequencesによる知的コミュニティ形成**: 300以上のブログ記事を体系化し、LessWrongという知的コミュニティを構築。コンテンツ発信による思想的リーダーシップの確立手法。AIプロダクトや事業においても、長文コンテンツによる思想的ポジショニングは有効
- **「P(doom)」に基づくリスク定量化フレーム**: 存亡リスクを確率として明示的に見積もり、それに基づいて行動を決定する。ガバナンス設計やAIプロダクトのリスク管理において、定量的リスク評価を意思決定の中心に据えるアプローチとして参考になる
- **独学者としてのキャリア構築**: 高校・大学を経ずに独学でAI安全性研究の創始者となった。公式な資格ではなく知的成果物の質で信頼を獲得する戦略は、起業家・テクノロジストにとって示唆的
- **政策転換の意思決定**: 技術研究が間に合わないと判断した時点で、組織の方向性を政策・コミュニケーションに大胆に転換。サンクコストに囚われない意思決定は、事業のピボット判断において参考になる
- **フィクションを通じた思想普及**: 『Harry Potter and the Methods of Rationality』で合理主義思想を大衆に広めた。教育・啓蒙においてフィクションを媒介にする戦略は、複雑な概念の普及手法として応用可能

## 9. 参考にすべきではない点

- **破滅確率の主観的見積もりの限界**: P(doom)の推定は本人の主観的判断に大きく依存し、検証可能な根拠に乏しい。「確率99%で人類滅亡」といった主張は、科学的実証が困難であり、政策議論においては慎重な扱いが必要
- **軍事的執行を含む提案の政治的非現実性**: データセンター空爆の提唱は、国際政治の現実を踏まえると実現可能性が極めて低く、むしろAI安全性議論の信頼性を毀損するリスクがある
- **独学者バイアス**: 学術的ピアレビューを経ない理論構築は、特定の認知的盲点を生む可能性がある。MIRIの技術研究が実質的に行き詰まった背景には、学術界との接続不足も指摘されている
- **過度な悲観論による行動麻痺**: 「何をしても無駄」という結論に至りやすい超悲観的フレームは、建設的な安全性研究や規制設計のモチベーションを削ぐ副作用がある
- **コミュニティの閉鎖性**: LessWrong/合理主義コミュニティは知的に高密度だが、外部からは「カルト的」との批判もあり、主張の社会的浸透に限界がある。エコーチェンバー化のリスク

---

### 関心クラスタ該当

| クラスタ | 該当 |
|---|---|
| 生成AI/LLM・エージェント・プロダクト化 | ○ AI整合性問題の理論的基盤を構築。LLMのアラインメント（RLHF等）の思想的源流。ただしプロダクト開発や商業化には直接関与していない |
| ガバナンス（制度設計・監査） | ◎ AI開発の国際的モラトリアム提唱、GPU規制の具体的提案、MIRIの政策転換。AI規制の制度設計議論において最も急進的な立場を代表 |
| 事業（DX・M&A・資本政策） | 該当なし |
| 思想（社会構造・ポストAGI） | ◎ ポストAGI世界観の最悲観シナリオを体系化。Coherent Extrapolated Volition、Sequences、合理主義運動を通じた社会思想への影響は甚大 |
| カルチャー（音楽・スポーツ） | 該当なし |
