# Geoffrey Hinton

> generated_at: 2026-02-25

## 1. 基本情報

- **人名**: Geoffrey Hinton（ジェフリー・ヒントン）
- **所属**: トロント大学 / Vector Institute
- **役職**: 名誉教授 / チーフ・サイエンティフィック・アドバイザー（元Google Brain）
- **誕生年**: 1947年

## 2. ラベル（大分類）

- 研究者

## 3. 小ラベル（テーマ軸）

- 生成AI
- 社会思想
- 教育

## 4. 概要

AIの存在論的リスクを警告する深層学習の父

## 5. 詳細

バックプロパゲーション普及・ボルツマンマシン発明で深層学習の基盤を築いた「AIのゴッドファーザー」。2018年チューリング賞、2024年ノーベル物理学賞受賞。2023年にGoogle退社し、AI存在論的リスクの警告活動に専念。

## 6. 行動原理（実現したい社会・信念）

- **根本信念**: 自分が発明した技術が人類存亡の脅威となりうるという危機感。「自分の人生の仕事を後悔している」と公言するほどの転向
- **目指す社会**: AIが人間より賢くなる前に安全性研究が追いつく世界。制御不能な超知能の出現を防ぐため、公的議論と規制が不可欠
- **意思決定原理**: 企業の利益から独立して発言するためにGoogleを退社。科学者としての良心を組織的制約より優先
- **哲学的姿勢**: 「10〜20%の確率で人類絶滅」という定量的リスク評価を提示。楽観論を排し、最悪シナリオへの備えを訴える

## 7. 興味深い主張

1. **「AIによる人類絶滅リスクは10〜20%」**: 現在のトレンドが続けば、AIが人間を超え制御不能なサブゴールを発展させるリスクがあると定量的に警告（sources.md #1）
2. **「自分の人生の仕事を後悔している」**: 深層学習の父が自らの研究成果の危険性を認め、公の場で後悔を表明（sources.md #2）
3. **「AIによるバイオテロが最大の短期的脅威」**: 悪意ある個人がAIを使って安価に致死的ウイルスを作成できるリスクを最も緊急の脅威として指摘（sources.md #3）
4. **「ビッグテックは存在論的リスクを過小評価している」**: 企業が利益追求のためにAIリスクを意図的に矮小化していると批判（sources.md #4）
5. **「AIは自身の制御不能なサブゴールを発展させうる」**: 人間が設定した目標とは異なる副次的目標をAIが自律的に追求し始める可能性を警告（sources.md #5）

## 8. 参考にできる点

- **「発明者自身によるリスク警告」の説得力モデル**: 技術の創造者が最もその危険性を理解しているという立場は、AIガバナンス・規制の議論において強力な説得フレーム
- **科学者の良心と組織からの独立**: Google退社という行動は、企業内で安全性を主張する限界と、独立した立場での発言の重要性を示す。組織内倫理の参照事例
- **定量的リスク評価フレーム**: 「10〜20%の絶滅確率」という数値化は、曖昧なリスク議論に具体性を与える手法。事業リスク評価にも応用可能な思考法
- **短期的脅威と長期的脅威の分離**: バイオテロ（短期）と超知能（長期）を分けて議論するフレームは、AI安全性の制度設計において優先順位付けに有用
- **後悔の公言の価値**: 公的に自分の過ちを認める姿勢は、知的誠実性の最高水準として、技術者倫理の参照モデル

## 9. 参考にすべきではない点

- **ドゥーマリズムの過度な一般化**: 10〜20%という数値に明確な根拠がなく、恐怖を煽るリスクがある。定量化の方法論自体が不透明
- **技術決定論的な見方**: AIが「必然的に」人間を超えるという前提は、技術の方向性が社会的選択に依存する点を軽視している可能性
- **規制万能論の限界**: 規制だけでリスクを管理できるという想定は、国際的な協調の困難さや執行の限界を考慮していない
- **LLMへの驚きに基づく転向**: GPT-4に「驚いた」ことが転向の契機であり、体系的なリスク分析に基づく結論かどうか疑問が残る

---

### 関心クラスタ該当

| クラスタ | 該当 |
|---|---|
| 生成AI/LLM・エージェント・プロダクト化 | ◎ バックプロパゲーション、ボルツマンマシン、深層学習の基盤技術 |
| ガバナンス（制度設計・監査） | ◎ AI存在論的リスク警告、企業独立した安全性主張、規制の必要性 |
| 事業（DX・M&A・資本政策） | △ Vector Institute設立（間接的） |
| 思想（社会構造・ポストAGI） | ◎ 人類絶滅リスク、超知能の制御不能性、科学者の倫理的責任 |
| カルチャー（音楽・スポーツ） | 該当なし |
